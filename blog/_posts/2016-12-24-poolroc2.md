---
layout: post
title:  Integrating ROC Curves, Model Ensembling and IDR
author: <a href="http://chandlerzuo.github.io/">Chandler</a>
---

**PROBLEM**

In the previous post, I proposed an method for pooling multiple ranked list from model-based probability estimations. The method was applicable when we have a binary classification task for a certain data set, with it being partitioned into multiple subsets, and different classification models built for each subset. Usually, this happens when we assume some clustering structure among observations in the original data set, and decide that it is the best to fit different classification models within each data set - an idea analogous to the concept of mixture modeling. 

In this post, I would like to discuss another related problem on pooling for binary classification models. When we have multiple classification models for the same data set, each of which produce a different probabilistic ranking of all observations in the data set, how should we pool model results in order to get the optimal ranking of all observations? Notice that in this case, we do not partition the data set into subset; each model is applied to the entire data set. As a result, for each observation, we have multiple estimates from different models, and we would like to combine all the different estimates together in order to produce a score that can provide the best ranking among observations.

This problem is closely linked to the concept of Ensemble Modeling, or [Ensemble Learning](https://en.wikipedia.org/wiki/Ensemble_learning), a very common strategy used in predictive modeling. The idea is that when we have different algorithms used for the same classification task. Rather than picking one "dictator" algorithm and disregard all the other algorithms, we would like to run a "poll" on the results of all different algorithms and produce a "democratic" result. This hopefully can reduce the prediction variability of using one single algorithm. In practice, ensembling methods are vastly useful. If you have [Kaggle](https://www.kaggle.com) competition experience, you must be conviced of this strategy. 

A key challenge, therefore, on model ensembling is to identify the best pooling strategy. Averaging predictions from different models is probably the most common choice, but is there any better way to do so? As a naive example, consider two observations A and B, and we build two binary classification models for them. Probability estimation for A under the two models is 0.1 and 0.9, while probability estimation for B is 0.4 and 0.6. The average probability estimation for A and B is both 0.5, but estimation for A has more variation. It is a tough decision to say which one is more likely to have Class 1 or Class 0.

**SOLUTION**

**DISCUSSION**

One better solution to this problem is by using [Irreproducible Discovery Rate](https://www.encodeproject.org/software/idr/)(IDR), a method proposed by Peter Bickel's team. Interestingly, this method was not proposed from the machine learning context, but from the context of the peak calling problem, the problem to identify protein binding regions on the genome. Protein binding status on the genome are binary, and in order to identify binding regions, multiple experiments are repeated under the same protocol to reduce the chance of idiosyncratic discoveries. When applying the same algorithm to different data sets, the classification probability estimates are different, and IDR offers a way to integrate the estimates from different experimental data to get a robust estimate. IDR was developed to pool results from the same algorithm of different data sets, while model ensembling deals with pooling results from different algorithms on the same data set. However, the fundamental problem underlying these two is the same: pooling multiple estimation results on the same set of observations. 

**CLAIMS**

*Works that applying the proposed methodology in this blog must follow explicit citation of this blog. Contact the author for more details.*

![](url)
