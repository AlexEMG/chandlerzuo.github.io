
Suppose we have $J$ different classification models, indexed by $1\leq
j\leq J$. For the $i$-th observation in model $j$, we denote the data
by $X_{ij}$, its true label by $\theta_{ij}\in\{0, 1\}$, and its
probabilistic estimate by $\hat\theta_j(X_{ij})\in [0, 1]$. The ROC
curve is thus generated by comparing all probabilistic estimates
$\hat\theta_j(X_{ij})$'s with their true labels $\theta_{ij}$'s. More
specifically, with a varying threshould $\lambda$, the theoretical FPR
$\alpha_j(\lambda)$ and the TPR $\beta_j(\lambda)$ are expressed as the following:

\alpha_j(\lambda)=P\{\hat\theta_j(X_{ij})\geq \lambda | \theta_{ij}=0\}

\beta_j(\lambda)=P\{\hat\theta_j(X_{ij})\geq \lambda | \theta_{ij}=1\}

By drawing a sequence of the above values taking at different $0\leq\lambda\leq 1$ on a 2D plot gives the ROC curve for model $j$.

To integrate the $J$ classifiers, we first assume that data coming from each of the $J$ models following a sampling distribution $(\pi_1, \cdots, \pi_J)$. In practice, $\pi_j$ is the proportion of observations from the $j$-th model. The integrate ROC curve can be calculated in the following way. First, notice that we are considering varying thresholds among different models, because the scale of different model estimates are not comparable. Let's take a threshold $\lambda_j$ for model $j$, and denote the vector $\Lambda=(\lambda_1,\cdots,\lambda_J)$. Then, we can compute the integrate FPR $\alpha(\Lambda)$ as well as TPR $\beta(\Lambda)$, where:

\alpha(\Lambda)=\sum_{j=1}^J\pi_jP\{\hat\theta_j(X_{ij})\geq \lambda_j | \theta_{ij}=0\}

\beta(\Lambda)=\sum_{j=1}^J\pi_jP\{\hat\theta_j(X_{ij})\geq \lambda_j | \theta_{ij}=1\}

With the above introduction, the problem statement is thus: what is
the optimal choice of $\Lambda$, such that given an FPR level
$\alpha_{\Lamba}\leq \alpha$, the TPR $\beta_{\Lamba}$ is maximized? Mathematically, this is:

\max \beta(\Lambda)=\sum_{j=1}^J\pi_jP\{\hat\theta_j(X_{ij})\geq \lambda_j | \theta_{ij}=1\}

s.t. \alpha(\Lambda)=\sum_{j=1}^J\pi_jP\{\hat\theta_j(X_{ij})\geq
\lambda_j | \theta_{ij}=0\} \leq \alpha

Consider the Lagrange multipler:

L(\Lambda) \\
= \sum_{j=1}^J\pi_jP\{\hat\theta_j(X_{ij})\geq \lambda_j | \theta_{ij}=1\} -L \sum_{j=1}^J\pi_jP\{\hat\theta_j(X_{ij})\geq \lambda_j | \theta_{ij}=0\} \\
= \sum_{j=1}^J\pi_j[\beta_j(\lambda_j) - L\alpha_j(\lambda_j)]

Taking partial derivative with each $\lambda_j$, we have that:

\frac{\partial \beta_j(\lambda_j) }{\partial \lambda_j}  = L \frac{\partial \alpha_j(\lambda_j)  }{\partial \lambda_j},

which implies

L = \frac{\partial \beta_j(\lambda_j) }{\partial \alpha_j(\lambda_j)} .

The last equation simply says that, the slope of different ROC curves
at their corresponding thresholds should be the same. For a fixed
FPR level $\alpha$, we can solve for $L=L_\alpha$ and
$\lambda_j=\lambda_{\alpha, j}$ such that the
integrate FPR is $\alpha$. This usually trivial unless the ROC
curve is not convex. Therefore, let:

\lambda_{j}(L)=\inf_{\lambda}\{\frac{\partial\beta_j(\lambda)}{\partial\alpha_j(\lambda)}<
L\}.

Substitute this into the function
$\sum\pi_{j}\alpha_{j}(\lambda_j(L))=\alpha$, we can solve for
$L=L_\alpha$.

The above analysis provides guidance to generate the integrate ROC curve
based on the optimal ranking. Notice that $L(\alpha)$ by our
construction is decreasing in $\alpha$ and is invertible. We can
express our integrate FPR $\alpha$ and integrate TPR $\beta$ both as functions of $L$:

\alpha(L)=\sum_{j=1}^J\pi_j\alpha_j(\lambda_j(L))

\beta(L)=\sum_{j=1}^J\pi_j\beta_j(\lambda_j(L)).

This also implies the optimal decision rule. Notice that $\alpha(L)$
and $\beta(L)$ are achieved when we make the rule
$r(X_{ij})=1\{\hat\theta_{ij}\geq \lambda_j(L)\}$. Let $P_j$ be the
projection $\Lambda \mapsto \lambda_j$. Define

\gamma(\theta)=\inf\{\frac{\partial\beta(L(\Lambda))}{\partial\alpha(L(\Lambda))}:
P_j(\Lambda)> \theta\}. 

Then it can be seen that the optimal decision rule

r(X_{ij})=1\{\gamma(\hat\theta_j(X_{ij})) > L\}.

This also shows that, if we are to pool observations from different
models together and rank their probabilistic score for label 1, we should rank according to the values
$\gamma(\hat\theta(X_{ij}))$.

Finally, we only need consider how to estimate $\gamma$ in
practice. Since the empirical ROC curve is a step function and can not
be differentiable, we need apply some smoothing method to get its
estimate. Suppose that such a smoothed function is obtained and each
point on the ROC curve can be expressed as $(\alpha'(\theta), \beta'(\theta))$. Then, for all probabilistic estimates from model $j$, we
order the probabilities as $0> \hat\theta(X_{ij})^{(1)} >
\hat\theta(X_{ij})^{(2)}> \cdots$ (w.l.o.g. we assume no ties in
these scores.) Define
$\hat\theta(X_{ij})^{(0)}=1$. The $\gamma$ value is thus estimated as:

\hat \gamma(\hat \theta(X_{ij})^{(k)})=\min_{1\leq l\leq k}\{[\beta'(\hat\theta_j(X_{ij})^{(l)})-\beta'(\hat\theta_j(X_{ij})^{(l
- 1)})]
/
[\alpha'(\hat\theta_j(X_{ij})^{(l)})-\alpha'(\hat\theta_j(X_{ij})^{(l-1)})]\}.
